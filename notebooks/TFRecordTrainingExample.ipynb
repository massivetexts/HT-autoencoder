{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from htrc_features import FeatureReader, utils\n",
    "import itertools\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103872"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = glob.glob('data/literature/ef/*.bz2')\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = (pd.read_csv('eng-vocab-1.txt.bz2', names=['token'])\n",
    "                .reset_index().set_index('token')\n",
    "                .to_dict())['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 books processed\n",
      "40 books processed\n",
      "60 books processed\n",
      "80 books processed\n",
      "100 books processed\n",
      "120 books processed\n",
      "140 books processed\n",
      "160 books processed\n",
      "180 books processed\n",
      "200 books processed\n",
      "220 books processed\n",
      "240 books processed\n",
      "260 books processed\n",
      "280 books processed\n",
      "300 books processed\n",
      "320 books processed\n",
      "340 books processed\n",
      "360 books processed\n",
      "380 books processed\n",
      "400 books processed\n",
      "420 books processed\n",
      "440 books processed\n",
      "460 books processed\n",
      "480 books processed\n",
      "500 books processed\n",
      "520 books processed\n",
      "540 books processed\n",
      "560 books processed\n",
      "580 books processed\n",
      "600 books processed\n",
      "620 books processed\n",
      "640 books processed\n",
      "660 books processed\n",
      "680 books processed\n",
      "700 books processed\n",
      "720 books processed\n",
      "740 books processed\n",
      "760 books processed\n",
      "780 books processed\n",
      "800 books processed\n",
      "820 books processed\n",
      "840 books processed\n",
      "860 books processed\n",
      "880 books processed\n",
      "900 books processed\n",
      "920 books processed\n",
      "940 books processed\n",
      "960 books processed\n",
      "980 books processed\n",
      "1000 books processed\n",
      "1020 books processed\n",
      "1040 books processed\n",
      "1060 books processed\n",
      "1080 books processed\n",
      "1100 books processed\n",
      "1120 books processed\n",
      "1140 books processed\n",
      "1160 books processed\n",
      "1180 books processed\n",
      "1200 books processed\n",
      "1220 books processed\n",
      "1240 books processed\n",
      "1260 books processed\n",
      "1280 books processed\n",
      "1300 books processed\n",
      "1320 books processed\n",
      "1340 books processed\n",
      "1360 books processed\n",
      "1380 books processed\n",
      "1400 books processed\n",
      "1420 books processed\n",
      "1440 books processed\n",
      "1460 books processed\n",
      "1480 books processed\n",
      "1500 books processed\n",
      "1520 books processed\n",
      "1540 books processed\n",
      "1560 books processed\n",
      "1580 books processed\n",
      "1600 books processed\n",
      "1620 books processed\n",
      "1640 books processed\n",
      "1660 books processed\n",
      "1680 books processed\n",
      "1700 books processed\n",
      "1720 books processed\n",
      "1740 books processed\n",
      "1760 books processed\n",
      "1780 books processed\n",
      "1800 books processed\n",
      "1820 books processed\n",
      "1840 books processed\n",
      "1860 books processed\n",
      "1880 books processed\n",
      "1900 books processed\n",
      "1920 books processed\n",
      "1940 books processed\n",
      "1960 books processed\n",
      "1980 books processed\n",
      "2000 books processed\n",
      "2020 books processed\n",
      "2040 books processed\n",
      "2060 books processed\n",
      "2080 books processed\n",
      "2100 books processed\n",
      "2120 books processed\n",
      "2140 books processed\n",
      "2160 books processed\n",
      "2180 books processed\n",
      "2200 books processed\n",
      "2220 books processed\n",
      "2240 books processed\n",
      "2260 books processed\n",
      "2280 books processed\n",
      "2300 books processed\n",
      "2320 books processed\n",
      "2340 books processed\n",
      "2360 books processed\n",
      "2380 books processed\n",
      "2400 books processed\n",
      "2420 books processed\n",
      "2440 books processed\n",
      "2460 books processed\n",
      "2480 books processed\n",
      "2500 books processed\n",
      "2520 books processed\n",
      "2540 books processed\n",
      "2560 books processed\n",
      "2580 books processed\n",
      "2600 books processed\n",
      "2620 books processed\n",
      "2640 books processed\n",
      "2660 books processed\n",
      "2680 books processed\n",
      "2700 books processed\n",
      "2720 books processed\n",
      "2740 books processed\n",
      "2760 books processed\n",
      "2780 books processed\n",
      "2800 books processed\n",
      "2820 books processed\n",
      "2840 books processed\n",
      "2860 books processed\n",
      "2880 books processed\n",
      "2900 books processed\n",
      "2920 books processed\n",
      "2940 books processed\n",
      "2960 books processed\n",
      "2980 books processed\n",
      "3000 books processed\n",
      "3020 books processed\n",
      "3040 books processed\n",
      "3060 books processed\n",
      "3080 books processed\n",
      "3100 books processed\n",
      "3120 books processed\n",
      "3140 books processed\n",
      "3160 books processed\n",
      "3180 books processed\n",
      "3200 books processed\n",
      "3220 books processed\n",
      "3240 books processed\n",
      "3260 books processed\n",
      "3280 books processed\n",
      "3300 books processed\n",
      "3320 books processed\n",
      "3340 books processed\n",
      "3360 books processed\n",
      "3380 books processed\n",
      "3400 books processed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3199\u001b[0m         \u001b[0;31m# value exception to occur first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3201\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_setitem_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_check_setitem_copy\u001b[0;34m(self, stacklevel, t, force)\u001b[0m\n\u001b[1;32m   2671\u001b[0m             \u001b[0;31m# the copy weakref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2672\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2673\u001b[0;31m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2674\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_referents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2675\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Split into 20 chunks. For the sample, that's about 230 books per file\n",
    "n_chunks = 500\n",
    "books_processed = 0\n",
    "\n",
    "for i in range(n_chunks):\n",
    "    filename = 'data/literature/tfrecords/ef-files-%d.tfrecord' % i\n",
    "    ef_files = FeatureReader(paths=paths[i::n_chunks])\n",
    "\n",
    "    with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "        for vol in ef_files.volumes():\n",
    "            # List of pages to include\n",
    "            page_list = [int(p.seq) for p in vol.pages() if {'en': '1.00'} in p.languages]\n",
    "\n",
    "            # Volume-wide token counts, filtered to page_list.\n",
    "            df = vol.tokenlist(section='body', case=False, pos=False).loc[page_list,].reset_index()\n",
    "            \n",
    "            # Key tokens with integer IDs and drop N/As\n",
    "            df['token_id'] = df['lowercase'].apply(lambda x: token_dict[x] if x in token_dict else None)\n",
    "                \n",
    "            df = df.dropna()\n",
    "            df['token_id'] = df['token_id'].astype(int)\n",
    "\n",
    "            for page_num, df in df.groupby('page'):\n",
    "                # An array where the first row is ids and the second row is counts\n",
    "                arr = df[['token_id', 'count']].astype(int).values.T\n",
    "\n",
    "                token_ids = tf.train.Feature(int64_list=tf.train.Int64List(value=arr[0]))\n",
    "                counts = tf.train.Feature(int64_list=tf.train.Int64List(value=arr[1]))\n",
    "                volid = tf.train.Feature(bytes_list=tf.train.BytesList(value=[vol.id.encode('utf-8')]))\n",
    "                page_seq = tf.train.Feature(int64_list=tf.train.Int64List(value=[page_num]))\n",
    "\n",
    "                example = tf.train.Example(\n",
    "                    features=tf.train.Features(feature=\n",
    "                                             {\n",
    "                                                 'page_seq': page_seq,\n",
    "                                                 'volid': volid,\n",
    "                                                 'token_ids': token_ids,\n",
    "                                                 'counts': counts\n",
    "                                             })\n",
    "                )\n",
    "\n",
    "                writer.write(example.SerializeToString())\n",
    "            books_processed += 1\n",
    "            \n",
    "            if books_processed % 20 == 0:\n",
    "                print(books_processed, \"books processed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
